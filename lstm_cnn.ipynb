{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      " ----------------now is arousal-----------\n",
      "803 train sequences\n",
      "201 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (803, 87)\n",
      "X_test shape: (201, 87)\n",
      "-------------now is lstm_cnn---------\n",
      "Train...\n",
      "Train on 803 samples, validate on 201 samples\n",
      "Epoch 1/10\n",
      "803/803 [==============================] - 12s - loss: 3.8513 - val_loss: 2.2600\n",
      "Epoch 2/10\n",
      "803/803 [==============================] - 12s - loss: 0.8591 - val_loss: 1.2096\n",
      "Epoch 3/10\n",
      "803/803 [==============================] - 12s - loss: 0.6373 - val_loss: 0.7199\n",
      "Epoch 4/10\n",
      "803/803 [==============================] - 12s - loss: 0.4745 - val_loss: 0.7430\n",
      "Epoch 5/10\n",
      "803/803 [==============================] - 12s - loss: 0.3635 - val_loss: 0.7920\n",
      "Epoch 6/10\n",
      "803/803 [==============================] - 12s - loss: 0.2564 - val_loss: 0.7596\n",
      "Epoch 7/10\n",
      "803/803 [==============================] - 12s - loss: 0.1909 - val_loss: 0.7802\n",
      "Epoch 8/10\n",
      "803/803 [==============================] - 12s - loss: 0.1514 - val_loss: 0.8059\n",
      "Epoch 9/10\n",
      "803/803 [==============================] - 12s - loss: 0.1211 - val_loss: 0.8275\n",
      "200/201 [============================>.] - ETA: 0sTest score: 0.827472466141\n",
      "MAE, Pearson_r\n",
      "803 train sequences\n",
      "201 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (803, 87)\n",
      "X_test shape: (201, 87)\n",
      "-------------now is lstm_cnn---------\n",
      "Train...\n",
      "Train on 803 samples, validate on 201 samples\n",
      "Epoch 1/10\n",
      "803/803 [==============================] - 11s - loss: 4.0540 - val_loss: 0.8455\n",
      "Epoch 2/10\n",
      "803/803 [==============================] - 12s - loss: 0.8414 - val_loss: 1.2280\n",
      "Epoch 3/10\n",
      "803/803 [==============================] - 12s - loss: 0.6521 - val_loss: 0.7593\n",
      "Epoch 4/10\n",
      "803/803 [==============================] - 12s - loss: 0.5049 - val_loss: 0.7683\n",
      "Epoch 5/10\n",
      "803/803 [==============================] - 12s - loss: 0.3944 - val_loss: 0.8985\n",
      "Epoch 6/10\n",
      "803/803 [==============================] - 12s - loss: 0.3065 - val_loss: 0.8088\n",
      "Epoch 7/10\n",
      "803/803 [==============================] - 12s - loss: 0.2329 - val_loss: 0.8251\n",
      "Epoch 8/10\n",
      "803/803 [==============================] - 12s - loss: 0.1885 - val_loss: 0.8904\n",
      "Epoch 9/10\n",
      "803/803 [==============================] - 12s - loss: 0.1608 - val_loss: 0.8681\n",
      "200/201 [============================>.] - ETA: 0sTest score: 0.868070611017\n",
      "MAE, Pearson_r\n",
      "803 train sequences\n",
      "201 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (803, 87)\n",
      "X_test shape: (201, 87)\n",
      "-------------now is lstm_cnn---------\n",
      "Train...\n",
      "Train on 803 samples, validate on 201 samples\n",
      "Epoch 1/10\n",
      "803/803 [==============================] - 12s - loss: 2.9112 - val_loss: 0.8650\n",
      "Epoch 2/10\n",
      "803/803 [==============================] - 12s - loss: 0.8035 - val_loss: 0.8176\n",
      "Epoch 3/10\n",
      "803/803 [==============================] - 12s - loss: 0.5447 - val_loss: 0.9599\n",
      "Epoch 4/10\n",
      "803/803 [==============================] - 12s - loss: 0.3949 - val_loss: 0.9727\n",
      "Epoch 5/10\n",
      "803/803 [==============================] - 12s - loss: 0.2704 - val_loss: 0.9767\n",
      "Epoch 6/10\n",
      "803/803 [==============================] - 12s - loss: 0.1980 - val_loss: 0.9455\n",
      "Epoch 7/10\n",
      "803/803 [==============================] - 12s - loss: 0.1591 - val_loss: 0.9150\n",
      "Epoch 8/10\n",
      "803/803 [==============================] - 12s - loss: 0.1188 - val_loss: 0.9364\n",
      "200/201 [============================>.] - ETA: 0sTest score: 0.936376926318\n",
      "MAE, Pearson_r\n",
      "803 train sequences\n",
      "201 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (803, 87)\n",
      "X_test shape: (201, 87)\n",
      "-------------now is lstm_cnn---------\n",
      "Train...\n",
      "Train on 803 samples, validate on 201 samples\n",
      "Epoch 1/10\n",
      "803/803 [==============================] - 12s - loss: 3.2838 - val_loss: 1.1875\n",
      "Epoch 2/10\n",
      "803/803 [==============================] - 12s - loss: 0.6507 - val_loss: 1.1318\n",
      "Epoch 3/10\n",
      "803/803 [==============================] - 12s - loss: 0.4576 - val_loss: 1.2446\n",
      "Epoch 4/10\n",
      "803/803 [==============================] - 12s - loss: 0.3185 - val_loss: 1.1594\n",
      "Epoch 5/10\n",
      "803/803 [==============================] - 12s - loss: 0.2238 - val_loss: 1.1538\n",
      "Epoch 6/10\n",
      "803/803 [==============================] - 12s - loss: 0.1764 - val_loss: 1.1802\n",
      "Epoch 7/10\n",
      "803/803 [==============================] - 12s - loss: 0.1300 - val_loss: 1.1910\n",
      "Epoch 8/10\n",
      "803/803 [==============================] - 12s - loss: 0.0979 - val_loss: 1.1810\n",
      "201/201 [==============================] - 0s     \n",
      "Test score: 1.18098049081\n",
      "MAE, Pearson_r\n",
      "803 train sequences\n",
      "201 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (803, 87)\n",
      "X_test shape: (201, 87)\n",
      "-------------now is lstm_cnn---------\n",
      "Train...\n",
      "Train on 803 samples, validate on 201 samples\n",
      "Epoch 1/10\n",
      "803/803 [==============================] - 12s - loss: 8.1688 - val_loss: 1.0722\n",
      "Epoch 2/10\n",
      "803/803 [==============================] - 12s - loss: 0.7217 - val_loss: 0.8859\n",
      "Epoch 3/10\n",
      "803/803 [==============================] - 12s - loss: 0.5601 - val_loss: 1.0291\n",
      "Epoch 4/10\n",
      "803/803 [==============================] - 12s - loss: 0.3812 - val_loss: 0.9617\n",
      "Epoch 5/10\n",
      "803/803 [==============================] - 12s - loss: 0.2907 - val_loss: 0.9637\n",
      "Epoch 6/10\n",
      "803/803 [==============================] - 12s - loss: 0.2022 - val_loss: 1.0624\n",
      "Epoch 7/10\n",
      "803/803 [==============================] - 12s - loss: 0.1590 - val_loss: 1.0295\n",
      "Epoch 8/10\n",
      "803/803 [==============================] - 12s - loss: 0.1280 - val_loss: 1.0287\n",
      "200/201 [============================>.] - ETA: 0sTest score: 1.02868963266\n",
      "MAE, Pearson_r\n",
      "average evaluate result:\n",
      "0.775 0.437\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.constraints import unitnorm\n",
    "from keras.layers.core import Reshape, Flatten, Merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Convolution1D, MaxPooling1D\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "import math\n",
    "from keras_input_data import make_idx_data\n",
    "from load_vai import loadVAI\n",
    "import _pickle as cPickle\n",
    "from metrics import continuous_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lstm_cnn(W):\n",
    "    \n",
    "    nb_filter = 15\n",
    "    filter_length = 30\n",
    "    pool_length = 6\n",
    "    \n",
    "    region_input = Input(shape=(maxlen,), dtype='int32', name='region_input')\n",
    "        ###这是一个逗号标志的输入的区域的句子，属于整个文章的一个区域。\n",
    "    x = Embedding(W.shape[0], W.shape[1], weights=[W], input_length=maxlen)(region_input)\n",
    "\n",
    "    lstm_output = LSTM(64, return_sequences=True, name='lstm')(x)  \n",
    "\n",
    "    region_conv = Convolution1D(nb_filter=nb_filter,\n",
    "                                    filter_length=filter_length,\n",
    "                                    border_mode='valid',\n",
    "                                    activation='relu',\n",
    "                                    subsample_length=1)(lstm_output)\n",
    "    region_max = MaxPooling1D(pool_length=pool_length)(region_conv)\n",
    "    region_vector = Flatten()(region_max)\n",
    "    textvector = Dense(64, activation='relu')(region_vector)\n",
    "    predictions = Dense(1, activation='linear')(textvector)\n",
    "    final_model = Model(region_input, predictions, name='model')\n",
    "    model=final_model\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = cPickle.load(open(\"mr.p\", \"rb\"))\n",
    "    revs, W, W2, word_idx_map, vocab = x[0], x[1], x[2], x[3], x[4]\n",
    "    print(\"data loaded!\")\n",
    "    sentences=[]\n",
    "    for rev in revs:\n",
    "        sentence = rev['text']\n",
    "        sentences.append(sentence)\n",
    "    idx_data = make_idx_data(sentences, word_idx_map)\n",
    "    #print(idx_data)\n",
    "\n",
    "    dim = 'V'\n",
    "    column = loadVAI(dim)\n",
    "    irony=column\n",
    "    maxlen = 87  # cut texts after this number of words (among top max_features most common words)\n",
    "    batch_size = 8\n",
    "\n",
    "    Y = np.array(irony)\n",
    "    Y = [float(x) for x in Y]\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = cross_validation.train_test_split(idx_data, Y, test_size=0.2,\n",
    "    #                                                                      random_state=2)\n",
    "    n_MAE=0\n",
    "    n_Pearson_r=0\n",
    "    n_Spearman_r=0\n",
    "    n_MSE=0\n",
    "    n_R2=0\n",
    "    n_MSE_sqrt=0\n",
    "    SEED = 42\n",
    "    n = 5  # repeat the CV procedure 5 times to get more precise results\n",
    "    for i in range(n):\n",
    "        # for each iteration, randomly hold out 20% of the data as CV set\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "            idx_data, Y, test_size=.20, random_state=i * SEED)\n",
    "\n",
    "        print(len(X_train), 'train sequences')\n",
    "        print(len(X_test), 'test sequences')\n",
    "\n",
    "        max_features = W.shape[0]  # shape of W: (13631, 300) , changed to 14027 through min_df = 3\n",
    "        # print(max_features)\n",
    "\n",
    "        print(\"Pad sequences (samples x time)\")\n",
    "        X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "        X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "        print('X_train shape:', X_train.shape)\n",
    "        print('X_test shape:', X_test.shape)\n",
    "\n",
    "    \n",
    "        model =lstm_cnn(W)\n",
    "        print('-------------now is lstm_cnn---------')\n",
    "        model.compile(loss='mse', optimizer='adagrad')  # loss function: mse\n",
    "        print(\"Train...\")\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        result = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,validation_data=(X_test, y_test),\n",
    "                           callbacks=[early_stopping])\n",
    "        score = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "        print('Test score:', score)\n",
    "        # experiment evaluated by multiple metrics\n",
    "        predict = model.predict(X_test, batch_size=batch_size).reshape((1, len(X_test)))[0]\n",
    "        #print(predict)\n",
    "        # print('Y_test: %s' % str(y_test))\n",
    "        # print('Predict value: %s' % str(predict))\n",
    "#         estimate=continuous_metrics(y_test, predict, 'prediction result:')\n",
    "#         # MSE, MAE, Pearson_r, R2, Spearman_r, MSE_sqrt\n",
    "      \n",
    "#         n_MAE += estimate[0]\n",
    "#         n_Pearson_r += estimate[1]\n",
    "       \n",
    "#     ndigit=3\n",
    "\n",
    "#     avg_MAE =  round(n_MAE/5, ndigit)\n",
    "#     avg_Pearson_r =  round(n_Pearson_r/5, ndigit)\n",
    " \n",
    "#     print('average evaluate result:')\n",
    "#     print(avg_MAE ,avg_Pearson_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
