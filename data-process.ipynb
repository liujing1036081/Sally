{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.constraints import unitnorm\n",
    "from keras.layers.core import Reshape, Flatten, Merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Convolution1D, MaxPooling1D\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "import math\n",
    "from keras_input_data import make_idx_data\n",
    "from load_vai import loadVAI\n",
    "import _pickle as cPickle\n",
    "from metrics import continuous_metrics\n",
    "from keras import backend as K\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imdb_cnn(W=None):\n",
    "    # Number of feature maps (outputs of convolutional layer)\n",
    "    N_fm = 20\n",
    "    # kernel size of convolutional layer\n",
    "    kernel_size =5\n",
    "    dims = 300  # 300 dimension\n",
    "    maxlen = 87  # maxlen of sentence\n",
    "    max_features = W.shape[0]\n",
    "    hidden_dims = 100\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features, dims, input_length=maxlen, weights=[W]))\n",
    "    #model.add(Dropout(0.2))\n",
    "\n",
    "    # we add a Convolution1D, which will learn nb_filter\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Convolution1D(nb_filter=N_fm,\n",
    "                            filter_length=kernel_size,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            ))\n",
    "    model.add(Dropout(0.4))\n",
    "    # we use standard max pooling (halving the output of the previous layer):\n",
    "    model.add(MaxPooling1D(pool_length=1))\n",
    "\n",
    "    # We flatten the output of the conv layer,\n",
    "    # so that we can add a vanilla dense layer:\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    print('----------- imdbcnn----------')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lstm(W):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(W.shape[0], W.shape[1], input_length=maxlen))\n",
    "    model.add(LSTM(200))  # try using a GRU instead, for fun\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    print('----------- lstm----------')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cnn_lstm(W):\n",
    "    nb_filter = 20\n",
    "    filter_length = 5\n",
    "    pool_length = 1\n",
    "    lstm_output_size = 60\n",
    "    p = 0.5\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(W.shape[0], W.shape[1], input_length=maxlen, weights=[W]))\n",
    "    model.add(Dropout(p))\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(lstm_output_size))\n",
    "    model.add(Dropout(p))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    print('----------- cnnlstm----------')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cnn(W):\n",
    "    \n",
    "    nb_filter = 10\n",
    "    filter_length =2\n",
    "    pool_length = 1\n",
    "    \n",
    "    region_input = Input(shape=(maxlen,), dtype='int32', name='region_input')\n",
    "        ###这是一个逗号标志的输入的区域的句子，属于整个文章的一个区域。\n",
    "    x = Embedding(W.shape[0], W.shape[1], weights=[W], input_length=maxlen)(region_input)\n",
    "\n",
    "    lstm_output = LSTM(64, return_sequences=True, name='lstm')(x)  \n",
    "\n",
    "    region_conv = Convolution1D(nb_filter=nb_filter,\n",
    "                                    filter_length=filter_length,\n",
    "                                    border_mode='valid',\n",
    "                                    activation='relu',\n",
    "                                    subsample_length=1)(lstm_output)\n",
    "    region_max = MaxPooling1D(pool_length=pool_length)(region_conv)\n",
    "    region_vector = Flatten()(region_max)\n",
    "    textvector = Dense(64, activation='relu')(region_vector)\n",
    "    predictions = Dense(1, activation='linear')(textvector)\n",
    "    final_model = Model(region_input, predictions, name='model')\n",
    "    model=final_model\n",
    "    print('----------- lstmcnn----------')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    x = cPickle.load(open(\"mr.p\", \"rb\"))\n",
    "    revs, W, W2, word_idx_map, vocab = x[0], x[1], x[2], x[3], x[4]\n",
    "    print(\"data loaded!\")\n",
    "    sentences=[]\n",
    "    for rev in revs:\n",
    "        sentence = rev['text']\n",
    "        sentences.append(sentence)\n",
    "    idx_data = make_idx_data(sentences, word_idx_map)\n",
    "    #print(idx_data)\n",
    "\n",
    "    dim = 'V'\n",
    "    column = loadVAI(dim)\n",
    "    irony=column\n",
    "    maxlen = 87  # cut texts after this number of words (among top max_features most common words)\n",
    "    batch_size = 8\n",
    "\n",
    "    # option = 'Irony'  # or Arousal,irony\n",
    "    Y = np.array(irony)\n",
    "    Y = [float(x) for x in Y]\n",
    "    # print(option + ' prediction.......................')\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = cross_validation.train_test_split(idx_data, Y, test_size=0.2,\n",
    "    #                                                                      random_state=2)\n",
    "    n_MAE=0\n",
    "    n_Pearson_r=0\n",
    "    n_Spearman_r=0\n",
    "    n_MSE=0\n",
    "    n_R2=0\n",
    "    n_MSE_sqrt=0\n",
    "    SEED = 42\n",
    "    ID=list(range(1,1006))\n",
    "    n = 5  # repeat the CV procedure 5 times to get more precise results\n",
    "   \n",
    "    X_train_0, X_test_0, y_train_0, y_test_0, ID_train_0, ID_test_0 = cross_validation.train_test_split(\n",
    "         idx_data, Y, ID, test_size=.20, random_state=2)\n",
    "    j=0\n",
    "    for i in range(n):\n",
    "        n_MAE=0\n",
    "        n_Pearson_r=0\n",
    "        \n",
    "        for i in range(n):\n",
    "            # for each iteration, randomly hold out 20% of the data as CV set\n",
    "            # X_train, X_test, y_train, y_test, ID_train, ID_test = cross_validation.train_test_split(\n",
    "            #     idx_data, Y, ID, test_size=.20, random_state=i * SEED)\n",
    "            y_test = y_test_0\n",
    "            y_train = y_train_0\n",
    "            X_test = X_test_0\n",
    "            X_train = X_train_0\n",
    "            ID_test = ID_test_0\n",
    "            ID_train = ID_train_0\n",
    "            if i>0:\n",
    "                y_test=y_train_0[:201]\n",
    "                y_train=y_train_0[201:]+y_test_0\n",
    "                X_test = X_train_0[:201]\n",
    "                X_train=X_train_0[201:]+X_test_0\n",
    "                ID_test = ID_train_0[:201]\n",
    "                ID_train = ID_train_0[201:]+ID_test_0\n",
    "                y_test_0 = y_test\n",
    "                y_train_0 = y_train\n",
    "                X_test_0 = X_test\n",
    "                X_train_0 = X_train\n",
    "                ID_test_0 = ID_test\n",
    "                ID_train_0 = ID_train\n",
    "\n",
    "            max_features = W.shape[0]  # shape of W: (13631, 300) , changed to 14027 through min_df = 3\n",
    "            # print(max_features)\n",
    "\n",
    "            #print(\"Pad sequences (samples x time)\")\n",
    "            X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "            X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "            #print('X_train shape:', X_train.shape)\n",
    "            #print('X_test shape:', X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "            model = lstm_cnn(W)\n",
    "\n",
    "            model.compile(loss='mae', optimizer='adagrad')  # loss function: mse\n",
    "            #print(\"Train...\")\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            result = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,validation_data=(X_test, y_test),\n",
    "                               callbacks=[early_stopping])\n",
    "            score = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "            #print('Test score:', score)\n",
    "            # experiment evaluated by multiple metrics\n",
    "            predict = model.predict(X_test, batch_size=batch_size).reshape((1, len(X_test)))[0]\n",
    "            #print(ID_test)\n",
    "            predict=list(predict)\n",
    "            #print(list(predict))\n",
    "            headers=['ID_test','pred']\n",
    "            with open('./irony_1005_2/V_lstmcnn'+str(j+1)+'_prediction.csv','a+',newline='') as f:\n",
    "                    write = csv.writer(f)\n",
    "                    write.writerow(headers)\n",
    "                    for i in range(0,201):\n",
    "                        write.writerow([ID_test[i],predict[i]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            estimate=continuous_metrics(y_test, predict, 'prediction result:')\n",
    "             # MSE, MAE, Pearson_r, R2, Spearman_r, MSE_sqrt\n",
    "\n",
    "            n_MAE += estimate[0]\n",
    "            n_Pearson_r += estimate[1]\n",
    "\n",
    "        ndigit=3\n",
    "\n",
    "        avg_MAE =  round(n_MAE/5, ndigit)\n",
    "        avg_Pearson_r =  round(n_Pearson_r/5, ndigit) \n",
    "\n",
    "        print('average evaluate result:')\n",
    "        print(avg_MAE ,avg_Pearson_r)\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
